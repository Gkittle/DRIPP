struct MDP
    Î³ # discount factor
    ğ’® # state space
    ğ’œ # action space
    T # transition function
    R # reward function
    TR # sample transition and reward
end

function (Ï€::EpsilonGreedyExploration)(model, s)
    ğ’œ, Ïµ = model.ğ’œ, Ï€.Ïµ
    if rand() < Ïµ
        return rand(ğ’œ)
    end
    Q(s,a) = lookahead(model, s, a)
    return argmax(a->Q(s,a), ğ’œ)
end

mutable struct QLearning
    ğ’® # state space (assumes 1:nstates)
    ğ’œ # action space (assumes 1:nactions)
    Î³ # discount
    Q # action value function
    Î± # learning rate
end

lookahead(model::QLearning, s, a) = model.Q[s,a]

function update!(model::QLearning, s, a, r, sâ€²)
    Î³, Q, Î± = model.Î³, model.Q, model.Î±
    Q[s,a] += Î±*(r + Î³*maximum(Q[sâ€²,:]) - Q[s,a])
    return model
end

function simulate(ğ’«::MDP, model, Ï€, h, s)
    for i in 1:h
        a = Ï€(model, s)
        sâ€², r = ğ’«.TR(s, a)
        update!(model, s, a, r, sâ€²)
        s = sâ€²
    end
end

Q = zeros(length(ğ’«.ğ’®), length(ğ’«.ğ’œ))
Î± = 0.2 # learning rate
model = QLearning(ğ’«.ğ’®, ğ’«.ğ’œ, ğ’«.Î³, Q, Î±)
Ïµ = 0.1 # probability of random action
Ï€ = EpsilonGreedyExploration(Ïµ)
k = 20 # number of steps to simulate
s = 1 # initial state
simulate(ğ’«, model, Ï€, k, s)